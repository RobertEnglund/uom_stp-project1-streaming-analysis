{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many of the Top 200 songs occur on both the US and Global charts? \n",
    "\n",
    "Timeframe: past 52 weeks\n",
    "\n",
    "Observations:\n",
    "1. 829 songs are common across both charts\n",
    " - This comprises 64% of the US chart and 72% of the Global chart <br>\n",
    "<br> \n",
    "- Although the US has fewer common songs in the Top 200, the common songs comprise a large percent of streams\n",
    " - Common songs comprise 90% of streams in the US and 84% of streams globally\n",
    " - Question:  Are there songs on either chart with high streams that do not cross over to the other chart?\n",
    " - Question:  If there are such songs, what differentiates them from songs that cross over?<br>\n",
    "<br>\n",
    "- There appear to be two different types of songs that are common to both charts:\n",
    " - Those with a high correlation of weeks in the chart (globally popular) and those with no correlation\n",
    " - Question: What are the attributes that distinguish the two subsets?\n",
    "\n",
    "Additional questions:\n",
    "- Is there a time factor indicating direction of influence (timing of when common songs show up on one chart vs the other)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies and \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# data files\n",
    "US_path = \"Combined CSVs/US_combined.csv\"\n",
    "Global_path = \"Combined CSVs/Global_combined.csv\"\n",
    "\n",
    "# Read the mouse data and the study results\n",
    "us_data = pd.read_csv(US_path)\n",
    "global_data = pd.read_csv(Global_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create unique Artist Track field and display column headers in US data\n",
    "us_data[\"ArtistTrack\"] = us_data[\"Artist\"] + us_data[\"Track Name\"]\n",
    "us_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp - value counts to determine if there are headers in the data rows\n",
    "us_data[\"Position\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp\n",
    "# drop headers in data and convert Streams to integer - US data\n",
    "#\n",
    "us_data.drop(us_data[us_data.Position == \"Position\"].index, inplace=True)\n",
    "us_data[\"Position\"] = us_data[\"Position\"].astype(int)\n",
    "us_data[\"Streams\"] = us_data[\"Streams\"].astype(int)\n",
    "us_data[\"Artist\"] = us_data[\"Artist\"].str.replace('$','S')\n",
    "us_data[\"Track Name\"] = us_data[\"Track Name\"].str.replace('$','S')\n",
    "\n",
    "print (f\"Records in US data after dropping duplicate headers: {len(us_data)}\")\n",
    "us_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp - display record counts and data types of rows in US data\n",
    "print (f\"Records in Global data: {len(global_data)}\")\n",
    "global_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create unique Artist Track field and display column headers in Global data\n",
    "global_data[\"ArtistTrack\"] = global_data[\"Artist\"] + global_data[\"Track Name\"]\n",
    "global_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp - value counts to determine if there are headers in the data rows\n",
    "global_data[\"Position\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp\n",
    "# drop headers in data and convert Streams to integer - Global data\n",
    "global_data.drop(global_data[global_data.Position == \"Position\"].index, inplace=True)\n",
    "global_data[\"Position\"] = global_data[\"Streams\"].astype(int)\n",
    "global_data[\"Streams\"] = global_data[\"Streams\"].astype(int)\n",
    "global_data[\"Artist\"] = global_data[\"Artist\"].str.replace('$','S')\n",
    "global_data[\"Track Name\"] = global_data[\"Track Name\"].str.replace('$','S')\n",
    "\n",
    "\n",
    "print (f\"Records in US data after dropping duplicate headers: {len(global_data)}\")\n",
    "global_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input counts:\")\n",
    "us_data_recs_in = len(us_data)\n",
    "global_data_recs_in = len(global_data)\n",
    "print(f\"rows in us: {us_data_recs_in}\")\n",
    "print(f\"rows in global: {global_data_recs_in}\")\n",
    "\n",
    "us_dates_in = us_data[\"Date\"].value_counts()\n",
    "global_dates_in = global_data[\"Date\"].value_counts()\n",
    "print(f\"unique dates in us: {len(us_dates_in)}\")\n",
    "print(f\"unique dates in global: {len(global_dates_in)}\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get current date and format to compare to date from input (mm.dd.yy)\n",
    "\n",
    "#\n",
    "# Don't think I need this cell, but keep for the time being - pull into reference do for future use\n",
    "#\n",
    "\n",
    "\n",
    "full_year = datetime.today().strftime('%Y')\n",
    "month_day = datetime.today().strftime('%m.%d')\n",
    "\n",
    "# examples for confirming how string parsing works - keep in place for the time being\n",
    "month_day2 = month_day[2:]\n",
    "month_day3 = month_day[:3]\n",
    "#print (f\"month_day: {month_day}\")\n",
    "#print (f\"month_day2: {month_day2}\")\n",
    "#print (f\"month_day3: {month_day3}\")\n",
    "\n",
    "todays_date = f\"{month_day}.{full_year[2:]}\"\n",
    "print (todays_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get date to use for pulling last 2 months of data\n",
    "#print(datetime.today())\n",
    "print (datetime.today() + timedelta(days=-60))\n",
    "\n",
    "compare_year = (datetime.today() + timedelta(days=-60)).strftime('%Y')\n",
    "compare_month_day = (datetime.today() + timedelta(days=-60)).strftime('%m.%d')\n",
    "\n",
    "compare_date = f\"{compare_year[2:]}.{compare_month_day}\"\n",
    "print(compare_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat date from input for sorting/comparison\n",
    "\n",
    "# US Data\n",
    "us_data[\"year\"] = [x.strip()[-2:] for x in us_data['Date']]\n",
    "# strip [5:] does the same thing as strip [-2:] with the Date field, use [:5] to pull first 5 characters\n",
    "# keep for now but move to reference doc for future use\n",
    "#us_data[\"month_day\"] = [x.strip()[5:] for x in us_data['Date']]\n",
    "us_data[\"month_day\"] = [x.strip()[:5] for x in us_data['Date']]\n",
    "us_data[\"reformatted date\"] = us_data[\"year\"] + \".\" + us_data[\"month_day\"]\n",
    "\n",
    "# Global Data\n",
    "global_data['Date'] = global_data['Date'].astype(str)\n",
    "global_data[\"year\"] = [x.strip()[-2:] for x in global_data['Date']]\n",
    "global_data[\"month_day\"] = [x.strip()[:5] for x in global_data['Date']]\n",
    "global_data[\"reformatted date\"] = global_data[\"year\"] + \".\" + global_data[\"month_day\"]\n",
    "\n",
    "global_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Counts after date conversion:\")\n",
    "print(\"\")\n",
    "\n",
    "us_data_recs_chk1 = len(us_data)\n",
    "global_data_recs_chk1 = len(global_data)\n",
    "print(f\"rows in us: {us_data_recs_chk1}\")\n",
    "print(f\"rows in global: {global_data_recs_chk1}\")\n",
    "\n",
    "us_dates_chk1 = us_data[\"Date\"].value_counts()\n",
    "global_dates_chk1 = global_data[\"Date\"].value_counts()\n",
    "print(f\"unique dates in us: {len(us_dates_chk1)}\")\n",
    "print(f\"unique dates in global: {len(global_dates_chk1)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "\n",
    "us_recs_aft_cnv = len(us_data[\"Date\"].value_counts())\n",
    "global_recs_aft_cnv = len(global_data[\"Date\"].value_counts())\n",
    "\n",
    "#print(f\"unique dates in us after date conversion: {len(len(us_data[\"Date\"].value_counts())}\")\n",
    "print(f\"unique dates in us after conversion: {us_recs_aft_cnv}\")\n",
    "print(f\"unique dates in global after conversion: {global_recs_aft_cnv}\")\n",
    "\n",
    "us_data_recs_1 = len(us_data)\n",
    "global_data_recs_1 = len(global_data)\n",
    "print(f\"rows in us: {us_data_recs_1}\")\n",
    "print(f\"rows in global: {global_data_recs_1}\")\n",
    "\n",
    "us_dates_1 = us_data[\"Date\"].value_counts()\n",
    "global_dates_1 = global_data[\"Date\"].value_counts()\n",
    "print(f\"unique dates in us: {len(us_dates_1)}\")\n",
    "print(f\"unique dates in global: {len(global_dates_1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify unique song/artists from the past 2 months: US & Global\n",
    "# For each unique song/artist identify min & max position and total streams\n",
    "# What's the total population of unique song/artists in each dataset?\n",
    "# How many unique song/artists are common between datasets?\n",
    "# What percent of total dataset do the common song/artists comprise?\n",
    "# Does statistical analysis of the datasets provide insight?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create summary dataframes for comparing US to Global\n",
    "\n",
    "us_groupby = us_data.groupby([\"ArtistTrack\"])\n",
    "us_track_df = pd.DataFrame(us_groupby[\"Artist\"].max())\n",
    "us_track_df[\"Track Name\"] = us_groupby[\"Track Name\"].max()\n",
    "us_track_df[\"Weeks in Top 200\"] = us_groupby[\"Position\"].count()\n",
    "us_track_df[\"Top Position\"] = us_groupby[\"Position\"].min()\n",
    "us_track_df[\"Low Position\"] = us_groupby[\"Position\"].max()\n",
    "us_track_df[\"Total Streams\"] = us_groupby[\"Streams\"].sum()\n",
    "\n",
    "global_groupby = global_data.groupby([\"ArtistTrack\"])\n",
    "global_track_df = pd.DataFrame(global_groupby[\"Artist\"].max())\n",
    "global_track_df[\"Track Name\"] = global_groupby[\"Track Name\"].max()\n",
    "global_track_df[\"Weeks in Top 200\"] = global_groupby[\"Position\"].count()\n",
    "global_track_df[\"Top Position\"] = global_groupby[\"Position\"].min()\n",
    "global_track_df[\"Low Position\"] = global_groupby[\"Position\"].max()\n",
    "global_track_df[\"Total Streams\"] = global_groupby[\"Streams\"].sum()\n",
    "\n",
    "global_track_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of songs common to both charts\n",
    "common_track_df = pd.merge(us_track_df, global_track_df, how='inner', on='ArtistTrack', suffixes=('_US', '_GL'))\n",
    "common_track_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_scatterchart(x_axis, y_axis, title, x_label, y_label):\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    plt.scatter(x_axis, y_axis, marker=\"o\", facecolors=\"darkblue\")\n",
    "    fig.suptitle(title, fontsize=12)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_scatter_and(x_axis, y_axis, title, x_label, y_label):\n",
    "# print scatter charter and regression analsysi\n",
    "\n",
    "    # make x_axis amenable to calculation of regression values\n",
    "    x_axis = np.asarray(x_axis)\n",
    "    \n",
    "    # run linregress to get components for correlaton coefficient and regression model\n",
    "    (slope, intercept, rvalue, pvalue, stderr) = linregress(x_axis,  y_axis)\n",
    "    regress_values = x_axis * slope + intercept\n",
    "\n",
    "    # calc correlation coefficient and format regression model equation\n",
    "    correlation_coefficient = round(rvalue**2, 2)\n",
    "    line_eq = \"y = \" + str(round(slope,2)) + \"x + \" + str(round(intercept,2))\n",
    "\n",
    "    # figure out where to place the regression model equation\n",
    "    test_val = (slope*max(x_axis) + intercept) - (slope*min(x_axis) + intercept)\n",
    "    x_pos = min(x_axis)\n",
    "    if test_val > 0:\n",
    "        if max(y_axis) > 25:\n",
    "            y_pos = max(y_axis) - 5\n",
    "        else:\n",
    "            y_pos = max(y_axis) - 1\n",
    "    else:\n",
    "        if max(y_axis) > 25:\n",
    "            y_pos = min(y_axis) + 5\n",
    "        else:\n",
    "            y_pos = min(y_axis) + 1\n",
    "\n",
    "    # format and print plot\n",
    "    fig = plt.figure()\n",
    "    plt.scatter(x_axis, y_axis, marker=\"o\", facecolors=\"#79abcd\")\n",
    "    plt.plot(x_axis,regress_values,\"#f70d30\")\n",
    "    plt.annotate(line_eq,(x_pos,y_pos),fontsize=15,color=\"#f70d30\",fontweight=\"bold\")\n",
    "    fig.suptitle(title, fontsize=12)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.show()\n",
    "    print(f'Correlation coefficient = {correlation_coefficient}')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate total streams\n",
    "one_billion = 1000000000\n",
    "one_million = 1000000\n",
    "\n",
    "# calculate raw totals\n",
    "total_streams_US = us_track_df[\"Total Streams\"].sum()\n",
    "total_streams_GL = global_track_df[\"Total Streams\"].sum()\n",
    "total_common_streams_US = common_track_df[\"Total Streams_US\"].sum()\n",
    "total_common_streams_GL = common_track_df[\"Total Streams_GL\"].sum()\n",
    "total_unique_streams_US = total_streams_US - total_common_streams_US\n",
    "total_unique_streams_GL = total_streams_GL - total_common_streams_GL\n",
    "\n",
    "# calculate print totals\n",
    "# US total streams\n",
    "if total_streams_US > one_billion:\n",
    "    denominator = one_billion\n",
    "    stream_label_US = \"billion\"\n",
    "else:    \n",
    "    denominator = one_million\n",
    "    stream_label_US = \"million\"\n",
    "\n",
    "total_streams_US_f = float(\"{:.1f}\".format(total_streams_US / denominator))\n",
    "common_streams_US_f = float(\"{:.1f}\".format(total_common_streams_US / denominator))\n",
    "total_unique_streams_US_f = float(\"{:.1f}\".format(total_unique_streams_US / denominator))\n",
    "\n",
    "\n",
    "# Global total streams\n",
    "if total_streams_GL > one_billion:\n",
    "    denominator = one_billion\n",
    "    stream_label_GL = \"billion\"\n",
    "else:    \n",
    "    denominator = one_million\n",
    "    stream_label_GL = \"million\"\n",
    "\n",
    "total_streams_GL_f = float(\"{:.1f}\".format(total_streams_GL / denominator))\n",
    "common_streams_GL_f = float(\"{:.1f}\".format(total_common_streams_GL / denominator))\n",
    "total_unique_streams_GL_f = float(\"{:.1f}\".format(total_unique_streams_GL / denominator))\n",
    "\n",
    "streams_bars_df = pd.DataFrame({\"US\":[total_common_streams_US, total_unique_streams_US],\n",
    "                                \"Global\":[total_common_streams_GL, total_unique_streams_GL]})\n",
    "\n",
    "#print(total_streams_GL)\n",
    "#print(total_streams_GL_f)\n",
    "#print(total_common_streams_GL)\n",
    "#print(common_streams_GL_f)\n",
    "#print(total_unique_streams_GL)\n",
    "#print(total_unique_streams_GL_f)\n",
    "#streams_bars_df.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate song counts\n",
    "total_songs_US = len(us_track_df)\n",
    "total_songs_GL = len(global_track_df)\n",
    "total_songs_common = len(common_track_df)\n",
    "\n",
    "total_unique_songs_US = total_songs_US - total_songs_common\n",
    "total_unique_songs_GL = total_songs_GL - total_songs_common\n",
    "\n",
    "print(total_unique_songs_GL)\n",
    "print(total_songs_common)\n",
    "print(total_songs_GL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate percentages\n",
    "common_streams_pct_US = float(\"{:.1f}\".format((total_common_streams_US / total_streams_US) * 100))\n",
    "unique_streams_pct_US = float(\"{:.1f}\".format(100 - common_streams_pct_US))\n",
    "\n",
    "common_streams_pct_GL = float(\"{:.1f}\".format((total_common_streams_GL / total_streams_GL) * 100))\n",
    "unique_streams_pct_GL = float(\"{:.1f}\".format(100 - common_streams_pct_GL))\n",
    "\n",
    "common_songs_pct_US = float(\"{:.1f}\".format((total_songs_common / total_songs_US) * 100))\n",
    "unique_songs_pct_US = float(\"{:.1f}\".format(100 - common_songs_pct_US))\n",
    "\n",
    "common_songs_pct_GL = float(\"{:.1f}\".format((total_songs_common / total_songs_GL) * 100))\n",
    "unique_songs_pct_GL = float(\"{:.1f}\".format(100 - common_songs_pct_GL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The US Top 200 chart contains {total_songs_US} songs totaling {total_streams_US_f} {stream_label_US} streams\")\n",
    "print(f\"The Global Top 200 chart contains {len(global_track_df)} songs totaling {total_streams_GL_f} {stream_label_GL} streams\")\n",
    "print(f\"{total_songs_common} songs are common across both charts\")\n",
    "print(\"\")\n",
    "\n",
    "Artist_with_most_songs = common_track_df[\"Artist_US\"].mode()\n",
    "if len(Artist_with_most_songs) == 1:\n",
    "    ArtistX = Artist_with_most_songs[0]\n",
    "    SongsX_df = common_track_df.loc[common_track_df[\"Artist_US\"] == Artist_with_most_songs[0]]\n",
    "    print(f\"The artist with the most songs on both charts is {Artist_with_most_songs[0]} with {len(SongsX_df)} songs\")\n",
    "\n",
    "# Identify songs with longest run in each chart\n",
    "longest_run_US = common_track_df[\"Weeks in Top 200_US\"].max()\n",
    "longest_run_GL = common_track_df[\"Weeks in Top 200_GL\"].max()\n",
    "longest_run_songs_US_df = common_track_df.loc[common_track_df[\"Weeks in Top 200_US\"] == longest_run_US]\n",
    "longest_run_songs_GL_df = common_track_df.loc[common_track_df[\"Weeks in Top 200_GL\"] == longest_run_GL]\n",
    "\n",
    "print(f\"There were {len(longest_run_songs_US_df)} songs that stayed in the US Top 200 for {longest_run_US} weeks\")\n",
    "print(f\"There were {len(longest_run_songs_GL_df)} songs that stayed in the Global Top 200 for {longest_run_US} weeks\")\n",
    "\n",
    "common_longest_df = pd.merge(longest_run_songs_US_df, longest_run_songs_GL_df, how='inner', on='ArtistTrack')\n",
    "len(common_longest_df)\n",
    "\n",
    "if longest_run_US > longest_run_GL:\n",
    "    print(f\"There were {len(common_longest_df)} songs that stayed in the both charts for {longest_run_US} weeks\")\n",
    "else:\n",
    "    print(f\"There were {len(common_longest_df)} songs that stayed in the both charts for {longest_run_GL} weeks\")\n",
    "\n",
    "# Weeks on US chart vs Weeks on Global chart - common songs\n",
    "print_scatter_and(common_track_df[\"Weeks in Top 200_US\"], common_track_df[\"Weeks in Top 200_GL\"], \"Weeks on Chart: US vs Global\", \"Weeks on US Chart\", \"Weeks on Global Chart\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observation:  Correlation of weeks on chart for common songs in general is not strong\n",
    "#               but there appears to be a subset of that have a very high correlation    \n",
    "# Question: Are there common attributes of the subset?  Could these be indicators of a song crossing over to another chart?\n",
    "#\n",
    "# Observation:  There are songs that have little to no correlation\n",
    "# Question: Are there attributes that indicate that a song will not cross over to another chart (e.g, language)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare US Streams to Global Songs\n",
    "songs_bars_df = pd.DataFrame({\"Common\":[common_songs_pct_US, common_songs_pct_GL],\n",
    "                              \"Unique\":[unique_songs_pct_US, unique_songs_pct_GL]})\n",
    "labels = [\"US\",\"Global\"]\n",
    "ax = songs_bars_df.plot.bar(stacked=True,title=\"Composition of US Songs vs Global Songs\")\n",
    "ax.set_xticklabels(labels,rotation=45)\n",
    "ax.set_ylabel(\"Percent\")\n",
    "\n",
    "print(f\"US common songs {common_songs_pct_US}%, unique songs {unique_songs_pct_US}%\")\n",
    "print(f\"Global common songs {common_songs_pct_GL}%, unique songs {unique_songs_pct_GL}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare US Streams to Global Streams\n",
    "streams_bars_df = pd.DataFrame({\"Common\":[common_streams_pct_US, common_streams_pct_GL],\n",
    "                                \"Unique\":[unique_streams_pct_US, unique_streams_pct_GL]})\n",
    "labels = [\"US\",\"Global\"]\n",
    "ax = streams_bars_df.plot.bar(stacked=True,title=\"Composition of US Streams vs Global Streams\")\n",
    "ax.set_xticklabels(labels,rotation=45)\n",
    "ax.set_ylabel(\"Percent\")\n",
    "\n",
    "print(f\"US common streams {common_streams_pct_US}%, unique streams {unique_streams_pct_US}%\")\n",
    "print(f\"Global common streams {common_streams_pct_GL}%, unique streams {unique_streams_pct_GL}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observation:  US songs comprise a smaller percent of songs in the Top 200, but a larger percent of streams\n",
    "# Question: How valid an indicator is stream counts that a song from the US Top 200 will appear in the Global Top 200?\n",
    "#           Of songs in the US Top 200 with high stream counts, how many cross into the Global Top 200?\n",
    "# Question: Is there and observable time factor between when a common song appears on one vs the other chart?\n",
    "#           Who is the influencer?\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
